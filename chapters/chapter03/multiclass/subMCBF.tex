\subsection{The algorithms of multiclass classification with Bandit Feedback}
\label{subsec:multiclassBF}
In the conventional supervised learning paradigm, the forecaster has access to a data set in which the true labels of the inputs are provided. Sometime, the environment just provide the partial feedback instead of full one. Such problems are natural being,  it's the bandit versions of multiclass prediction problems. 

Here, we denote the number of classes is $K$, and by $\examples$  the sequence of training examples received over trials, where $x_i\in \mathbb{R}^d$ and $T$ is the number of training instances. In each trial, we denote the prediction by $\hy\in \{1,\dots,K\}$. Unlike the classical setup of online learning where an oracle provides the true class label $y_i\in \{1,\dots,K\} $ to the learner, in the case of partial feedback, the learner only receives one bit to response whether the prediction equals to the true label, i.e., $\1[y_t = \hy]$. The Bandit feedback is an application of Contextual Bandit with side information. 
So the goal of this problem is not only to minimize the error bound, but also to keep balance between Exploration and Exploitation. Some popular strategies of this issue have been introduced in Section~\ref{sec:tradeoff}, i.e. UCB, Thompson, $\epsilon$-greedy etc.  To apply trade-off strategies to this issue, we should understand the relationship between the prediction $\hy$ and the label set $\mathscr{Y}$. The prediction $\hy$ is the result of exploitation by the past information, it is the optimal arm or sub-optimal depends on the hypothesis. Unlike the supervised learning, we have no knowledge about the true label of $y_t$. So, it's necessary to sample other labels to explore more information of the label setting.

In this section, we will introduce a few traditional multiclass classification on combining the Bandit policies. 
 
\vspace{3ex}
\textbf{Banditron} \cite{kakade2008efficient} (see in Appendix~\ref{algo:banditron}), is a simple but effective learning strategy for online classification with bandit feedback, which is based on the algorithm Perceptron. Despite its age and simplicity, the Perceptron has proven to be quite effective in practical problems (more details about Perceptron see the previous section or \cite{rosenblatt1958perceptron} ). 
 
Similar to the Perceptron, at each round, the prediction $\hat{y}_t$ can be the best label according to the current weight matrix $w$, i.e. $\hy = \argmaxi<w_{t,i},x_t>$ . Mostly, Banditron exploits the quality of the current weight matrix to predict the label $\hy$. Unlike the Perceptron, if $\hy \neq y_t$, then it's difficult  to make an update since it's blind to the identity of $y_t$. Roughly speaking, it is difficult to learn when to exploit using $w_t$ . Since that, on some of rounds it's necessary to let the algorithm explore and uniformly predict a random label from the label set $\mathscr{Y}$. It's denoted by $\ty$ the predicted label. On rounds, in which it explores, (where $\ty \neq \hy$), if the forecaster additionally receives a positive feedback, i.e. $\ty = y_t$, then it indirectly obtains the full information regarding the identity of $y_t$, therefore it could update the weight matrix using this positive instance. The parameter $\epsilon$ controls the exploration-exploitation tradeoff, this is the $\epsilon$-Greedy strategy (refer to the Section~\ref{subsec:greedy}. 

The above intuitive argument is formalized by defining the update matrix $\tilde{U}_t$ to be a function of the randomized prediction $\ty$. We emphasize that $\tilde{U}_t$ accesses the correct label $y_t$ only through the indicator $\1[y_t = \ty] $ and is thus adequate for the bandit setting. Kakade\cite{kakade2008efficient} show that the expected value of the Banditron's  update matrix $\tilde{U}_t$ is exactly the Perceptron's update matrix $U_t$.  Banditron, the linearpredictor with $\epsilon$-Greedy strategy in bandit setting could be bounded for the regret bound $O(T^{2/3})$ compared to the hinge loss.

\vspace{3ex}
\textbf{Confidit}\cite{MCBFCRAMMER}, with the different strategy of tradeoff to Banditron, Confidit uses an alternative approach UCB( see in Section~\ref{subsec:ucb}), which is to maintain additional confidence information about the predictions. Specifically, given an input $\xt$, the algorithm not only computes score values, but also non-negative uncertainty values for these scores, denotes by $\epsilon_{i,t}$ an upper bound of confidence interval. Intuitively, high values of $\epsilon_{i,t}$ indicate that the algorithm is less confident in the value of the score $w_i^T \xt$. Given a new example, the algorithm outputs the label with the highest upper confidence bound (UCB), computed as the sum of score and uncertainty as following, 
\[\hy = \argmaxi (\mathbf{w}_i^T \xt + \epsilon_{i,t}).\] 

Intuitively, a label $\hy$ is output by the algorithm if either its score is high or the uncertainty in predicting is high, and there is necessary to obtain information about it.  Specifically, this algorithm is based on the Second Order Perceptron, it maintains is a positive semi-definite matrix per label, $A_{i,t}\in \mathbb{R}^{d\times d}$ to compute the upper confidence to each label. More details of this algorithm described in Appendix~\ref{algo:confidit}.  Confidit develops the Second Order Perceptron with UCB strategy to solve the supervised problem with Bandit feedback. And it uses the correlation matrix to estimate the uncertainty of label set. Its regret bound of $O(\sqrt{T}\log{T})$, which is more excellent than the one of Banditron.