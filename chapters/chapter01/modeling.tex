\section{Modeling of Bandit Problems}
\label{sec:modeling}

Consider the Robbins's sequential decision experiment\cite{robbins1952bandit}, there are several possible choices (finite arms or infinite arms). Generally, we should make a choice from the set of arms at each time. After observation, you will receive a boolean feedback $\mathbf{1}$ or $\mathbf{0}$  as reward. 
Whatever the feedback is, it is always the useful knowledge to make prediction in future from the past. We can even estimate the fixed but unknown distribution of arms by the reward of each time. The goal of bandit problems, is to maximize the received rewards from all steps, minimize the cumulative regret and find the optimal arm (the set of optimal arms). 

The bandit problem can be defined by the number of trials, as the ``finite horizon'' or ``infinite horizon''. The former has a number of trials fixed and small, the latter's trials number usually unknown in advance, but has some probability at any trial will be the last; by the number of arms, it could be divided into ``two arms'', ``K-armed'' and ``Many-armed'' Bandits; by the environment, it is divided into ``Stationary bandit'', ``Non-stationary bandit'', ``Adversary bandit'' and ``Contextual bandit''.

In the next chapters, we will tell about each kind bandits, their performance, properties and caracteristics. Specially, we will emphasize on Chapter~\ref{chap:BF} and Chapter~\ref{chap:momab}, to propose the novel solutions and algorithms. Here, we address a simple process in order to introduce the core thought of Bandit problems. Just like we have introduced , the target of bandit problems is to maximize the number of rewards over all steps. Generally, in the first few trials, pursuing the goal might involve trying different arms, getting some knowledge of which arms may be rewarding or which may not. Towards to the end, it will become increasingly important to pull the same  arm repeatedly by the same reasonable strategy. Where the way to check out some uncertain arms is called ``Exploration'', and the ways to choose the arm (a set of arms) that is more certain by the reasonable strategy is called ``Exploitation''. 
The decisions are always depending on the potential knowledge of the optimal arms in the setting. If the setting is believed to have optimal arms, then others will be encouraged to try of arms. On the other hand, if the setting has a bad reputation for arm quality, a modest reward rate encourages repeat choice.

Performing well on bandit problems, requires to keep a balance between Exploration and Exploitation during decisions. In early steps, it makes sense to explore, to search for those with the highest reward rates. In later times, it makes sense to exploit those arms known to be good, maximizing the reward for each current decision. How to keep balance between Exploration and Exploitation, ( the details shown in the Chapter~\ref{chap:MAB} ), it should be influenced by factors such as the distribution of reward rates, the total number of plays, etc.

We also attempt to construct the bandit models that capture individual differences and collective performance in solving bandit problems. Differences in how people solve the bandit problems that involve optimization and classification under interactive constraints, provide a window onto variance in cognitive abilities. Recently, take attention to some mechanism, a process of taking into account the collective ability of a group of individuals to solve a task, has discovered that a large group's aggregated answers to questions involving quantity estimation and general world knowledge are as good as, and often better than, the answer given by any of the individuals within the group (e.g.\cite{surowiecki2005wisdom}). We are thus interested in whether this phenomenon also applies to sequential tasks such as bandit problems whether a collective decision-making through aggregating methods can achieve better outcomes than most of the individuals.

From the theorem view, we also care about how to find optimal solutions for bandit problems that make model evaluation and model comparisons more efficient. We dedicate next chapters of this dissertation to our effort on this problem.